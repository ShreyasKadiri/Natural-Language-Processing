{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification using spaCy v3.0 transformers in Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlm0t9Op5tACApE9H0qkDJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyasKadiri/Natural-Language-Processing/blob/main/Text_Classification_using_spaCy_v3_0_transformers_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBYlUR2u7DJh"
      },
      "source": [
        "#Import all required libraries\n",
        "import spacy\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "import sys\n",
        "from spacy import displacy\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from spacy.tokens import DocBin"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlKyYvmn8glg"
      },
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def remove_url(text): \n",
        "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return url_pattern.sub(r'', text)\n",
        " # converting return value from list to string\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text ): \n",
        "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
        "    delete_dict[' '] = ' ' \n",
        "    table = str.maketrans(delete_dict)\n",
        "    text1 = text.translate(table)\n",
        "    #print('cleaned:'+text1)\n",
        "    textArr= text1.split()\n",
        "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n",
        "    \n",
        "    return text2.lower()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLiipEQ28n21"
      },
      "source": [
        "def make_docs(file_path):\n",
        "    \"\"\"\n",
        "    this will take a list of texts and labels \n",
        "    and transform them in spacy documents\n",
        "    \n",
        "    data: list(tuple(text, label))\n",
        "    \n",
        "    returns: List(spacy.Doc.doc)\n",
        "    \"\"\"\n",
        "    train_data = pd.read_csv(file_path)\n",
        "    train_data.dropna(axis = 0, how ='any',inplace=True) \n",
        "    train_data['Num_words_text'] = train_data['text'].apply(lambda x:len(str(x).split())) \n",
        "    mask = train_data['Num_words_text'] >2\n",
        "    train_data = train_data[mask]\n",
        "    print(train_data['sentiment'].value_counts())\n",
        "    \n",
        "    train_data['text'] = train_data['text'].apply(remove_emoji)\n",
        "    train_data['text'] = train_data['text'].apply(remove_url)\n",
        "    train_data['text'] = train_data['text'].apply(clean_text)\n",
        "   \n",
        "    data = tuple(zip(train_data['text'].tolist(), train_data['sentiment'].tolist())) \n",
        "    print(data[1])\n",
        "    docs = []\n",
        "    # nlp.pipe([texts]) is way faster than running \n",
        "    # nlp(text) for each text\n",
        "    # as_tuples allows us to pass in a tuple, \n",
        "    # the first one is treated as text\n",
        "    # the second one will get returned as it is.\n",
        "    nlp = spacy.load(\"en_core_web_trf\")\n",
        "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
        "        \n",
        "        # we need to set the (text)cat(egory) for each document\n",
        "        #print(label)\n",
        "        if (label=='positive'):\n",
        "            doc.cats['positive'] = 1\n",
        "            doc.cats['negative'] = 0\n",
        "            doc.cats['neutral']  = 0\n",
        "        elif (label=='negative'):\n",
        "            doc.cats['positive'] = 0\n",
        "            doc.cats['negative'] = 1\n",
        "            doc.cats['neutral']  = 0\n",
        "        else:\n",
        "            doc.cats['positive'] = 0\n",
        "            doc.cats['negative'] = 0\n",
        "            doc.cats['neutral']  = 1\n",
        "        #print(doc.cats)\n",
        "        \n",
        "        # put them into a nice list\n",
        "        docs.append(doc)\n",
        "    \n",
        "    return docs,train_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgjVy9AC832W"
      },
      "source": [
        "train_docs,train_data  = make_docs(\"C:\\\\TweetSenitment\\\\train.csv\")\n",
        "# then we save it in a binary file to disc\n",
        "doc_bin = DocBin(docs=train_docs)\n",
        "doc_bin.to_disk(\"./textcat_data/textcat_train.spacy\")\n",
        "\n",
        "test_docs,test_data  = make_docs(\"C:\\\\TweetSenitment\\\\test.csv\")\n",
        "# then we save it in a binary file to disc\n",
        "doc_bin = DocBin(docs=test_docs)\n",
        "doc_bin.to_disk(\"./textcat_data/textcat_valid.spacy\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H72cGh3187hg"
      },
      "source": [
        "!python -m spacy init fill-config ./textcat_base_config.cfg ./textcat_config.cfg"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2oKOpcx8-Oy"
      },
      "source": [
        "!python -m spacy train textcat_config.cfg --verbose --output ./textcat_output --paths.train textcat_data/textcat_train.spacy --paths.dev textcat_data/textcat_valid.spacy"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF6yunY89AeC"
      },
      "source": [
        "nlp_textcat = spacy.load(\"textcat_output/model-best\")\n",
        "test_texts = test_data['text'].tolist()\n",
        "test_cats = test_data['sentiment'].tolist()\n",
        "doc2 = nlp_textcat(test_texts[100])\n",
        "print(\"Text: \"+ test_texts[100])\n",
        "print(\"Orig Cat:\"+ test_cats[100])\n",
        "print(\" Predicted Cats:\") \n",
        "print(doc2.cats)\n",
        "print(\"=======================================\")\n",
        "doc2 = nlp_textcat(test_texts[1000])\n",
        "print(\"Text: \"+ test_texts[1000])\n",
        "print(\" Orig Cat:\"+test_cats[1000])\n",
        "print(\" Predicted Cats:\") \n",
        "print(doc2.cats)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmG0czrR9EhD"
      },
      "source": [
        "doc2 = nlp_textcat(\"Avengers Endgame was a great movie\")\n",
        "print(doc2.cats)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYIaE4PF9H3b"
      },
      "source": [
        "doc2 = nlp_textcat(\"Data science is tough to master\")\n",
        "print(doc2.cats)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}